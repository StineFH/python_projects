{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math  # Just ignore this :-)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML E2021 - Week 10 - Practical Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "22a39008-e542-438a-83f4-fd54f37017b8"
    }
   },
   "source": [
    "In the exercise below, you will see an example of how a hidden Markov model (HMM)\n",
    "can be represented, and you will implement and experiment with the computation of the joint probability and various decodings as explained in the lectures in week 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "13722c25-cdc1-4ea4-98f0-a779946ce53c"
    }
   },
   "source": [
    "# 1 - Representing an HMM\n",
    "\n",
    "We can represent a HMM as a triple consisting of three matrices: a $K \\times 1$ matrix with the initial state probabilities, a $K \\times K$ matrix with the transition probabilities and a $K \\times |\\Sigma|$ matrix with the emission probabilities. In Python we can write the matrices like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "b9f24259-216a-438c-86e7-aad8cb275bc6"
    }
   },
   "outputs": [],
   "source": [
    "init_probs_7_state = np.array([0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00])\n",
    "\n",
    "trans_probs_7_state = np.array([\n",
    "#     0     1      2     3     4     5     6\n",
    "    [0.00, 0.00, 0.90, 0.10, 0.00, 0.00, 0.00],\n",
    "    [1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
    "    [0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
    "    [0.00, 0.00, 0.05, 0.90, 0.05, 0.00, 0.00],\n",
    "    [0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00],\n",
    "    [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00],\n",
    "    [0.00, 0.00, 0.00, 0.10, 0.90, 0.00, 0.00],\n",
    "])\n",
    "\n",
    "emission_probs_7_state = np.array([\n",
    "    #   A     C     G     T\n",
    "    [0.30, 0.25, 0.25, 0.20],\n",
    "    [0.20, 0.35, 0.15, 0.30],\n",
    "    [0.40, 0.15, 0.20, 0.25],\n",
    "    [0.25, 0.25, 0.25, 0.25],\n",
    "    [0.20, 0.40, 0.30, 0.10],\n",
    "    [0.30, 0.20, 0.30, 0.20],\n",
    "    [0.15, 0.30, 0.20, 0.35],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0fb33618-de36-44df-8462-14b7e58d3b4b"
    }
   },
   "source": [
    "How do we use these matrices? Remember that we are given some sequence of observations, e.g. like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "ed50140f-2197-43fc-837a-e3799ffe5904"
    }
   },
   "outputs": [],
   "source": [
    "obs_example = 'GTTTCCCAGTGTATATCGAGGGATACTACGTGCATAGTAACATCGGCCAA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2bef7e67-d545-447f-9d0c-7b6078ee627b"
    }
   },
   "source": [
    "To make a lookup in our three matrices, it is convenient to translate each symbol in the string to an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbpresent": {
     "id": "ed1f0acd-83cf-451a-ac94-957ebd8a0d87"
    }
   },
   "outputs": [],
   "source": [
    "def translate_observations_to_indices(obs):\n",
    "    mapping = {'a': 0, 'c': 1, 'g': 2, 't': 3}\n",
    "    return [mapping[symbol.lower()] for symbol in obs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6fcef106-fbc3-4f3d-ab54-5d89271eedf0"
    }
   },
   "source": [
    "Let's try to translate the example above using this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbpresent": {
     "id": "8604d821-64fb-49e5-8c98-a791c6d42831"
    }
   },
   "outputs": [],
   "source": [
    "obs_example_trans = translate_observations_to_indices(obs_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function below to translate the indices back to observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_indices_to_observations(indices):\n",
    "    mapping = ['a', 'c', 'g', 't']\n",
    "    return ''.join(mapping[idx] for idx in indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gtttcccagtgtatatcgagggatactacgtgcatagtaacatcggccaa'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_indices_to_observations(translate_observations_to_indices(obs_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now each symbol has been transformed (predictably) into a number which makes it much easier to make lookups in our matrices. We'll do the same thing for a list of states (a path):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_path_to_indices(path):\n",
    "    return list(map(lambda x: int(x), path))\n",
    "\n",
    "def translate_indices_to_path(indices):\n",
    "    return ''.join([str(i) for i in indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a path through an HMM, we can now translate it to a list of indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_example = '33333333333321021021021021021021021021021021021021'\n",
    "\n",
    "#translate_path_to_indices(path_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "cf3f4b33-715a-443a-a933-17f29d3ffa75"
    }
   },
   "source": [
    "Finally, we can collect the three matrices in a class to make it easier to work with our HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbpresent": {
     "id": "43336c35-2a6f-46e9-b86e-0367377dca39"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.9 , 0.1 , 0.  , 0.  , 0.  ],\n",
       "       [1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.05, 0.9 , 0.05, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ],\n",
       "       [0.  , 0.  , 0.  , 0.1 , 0.9 , 0.  , 0.  ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class hmm:\n",
    "    def __init__(self, init_probs, trans_probs, emission_probs):\n",
    "        self.init_probs = init_probs\n",
    "        self.trans_probs = trans_probs\n",
    "        self.emission_probs = emission_probs\n",
    "\n",
    "# Collect the matrices in a class.\n",
    "hmm_7_state = hmm(init_probs_7_state, trans_probs_7_state, emission_probs_7_state)\n",
    "\n",
    "# We can now reach the different matrices by their names. E.g.:\n",
    "hmm_7_state.trans_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing, here's another model (which we will refer to as the 3-state model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_probs_3_state = np.array([0.10, 0.80, 0.10])\n",
    "\n",
    "trans_probs_3_state = np.array([\n",
    "    [0.90, 0.10, 0.00],\n",
    "    [0.05, 0.90, 0.05],\n",
    "    [0.00, 0.10, 0.90],\n",
    "])\n",
    "\n",
    "emission_probs_3_state = np.array([\n",
    "    #   A     C     G     T\n",
    "    [0.40, 0.15, 0.20, 0.25],\n",
    "    [0.25, 0.25, 0.25, 0.25],\n",
    "    [0.20, 0.40, 0.30, 0.10],\n",
    "])\n",
    "\n",
    "hmm_3_state = hmm(init_probs_3_state, trans_probs_3_state, emission_probs_3_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "69bf9e07-95ad-429f-b85b-dbb8c1b6c310"
    }
   },
   "source": [
    "# 2 - Validating an HMM (and handling floats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4c1bd08c-2c49-45e6-81e7-5bbbbd2f799c"
    }
   },
   "source": [
    "Before using the model we'll write a function to validate that the model is valid. That is, the matrices should have the right dimensions and the following things should be true:\n",
    "\n",
    "1. The initial probabilities must sum to 1.\n",
    "2. Each row in the matrix of transition probabilities must sum to 1.\n",
    "3. Each row in the matrix of emission probabilities must sum to 1.\n",
    "4. All numbers should be between 0 and 1, inclusive.\n",
    "\n",
    "Write a function `validate_hmm` that given a model returns True if the model is valid, and False otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbpresent": {
     "id": "2c6c3d32-db85-482b-970c-9bcacd9f5b32"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def validate_hmm(model):\n",
    "    K = len(model.init_probs) # Dim of init_probs\n",
    "    \n",
    "    if (len(model.trans_probs) == K and  \n",
    "        all(elm == K for elm in list(map(len, model.trans_probs))) and  # Dims of trans_probs\n",
    "        \n",
    "        len(model.emission_probs) == K and  # Dims of emission_probs\n",
    "        \n",
    "        math.isclose(np.sum(model.init_probs), 1) and  # Initial probs sum to 1\n",
    "        \n",
    "        all(math.isclose(elm, 1) for elm in list(map(sum, model.trans_probs))) and  # Each row in trans_probs sum to 1\n",
    "        all(math.isclose(elm, 1) for elm in list(map(sum, model.emission_probs))) and  # Each row in emission_probs sum to 1\n",
    "        \n",
    "        # All numbers are between 0 and 1\n",
    "        all(0<=elm<=1 for elm in model.init_probs) and \n",
    "        all(0<=elm<=1 for lst in model.trans_probs for elm in lst) and \n",
    "        all(0<=elm<=1 for lst in model.emission_probs for elm in lst)):\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    else: \n",
    "        return False\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "05430599-405e-4ffa-add6-84288abe4364"
    }
   },
   "source": [
    "We can now use this function to check whether the example model is a valid model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "nbpresent": {
     "id": "1e3e74d8-7951-4362-af49-240b7d90532d"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_hmm(hmm_7_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might run into problems related to summing floating point numbers because summing floating point numbers does not (always) give the expected result as illustrated by the following examples. How do you suggest to deal with this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.15 + 0.30 + 0.20 + 0.35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order of the terms matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.20 + 0.35 + 0.15 + 0.30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it changes the prefix sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44999999999999996"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.15 + 0.30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7000000000000001"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.20 + 0.35 + 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44999999999999996"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.15 + 0.30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On should never compare floating point numbers. They represent only an 'approximation'. Read more about the 'problems' in 'What Every Computer Scientist Should Know About Floating-Point Arithmetic' at:\n",
    "\n",
    "http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "7b9b636c-c901-4ea8-a99d-190a396b2405"
    }
   },
   "source": [
    "# 3 - Computing the Joint Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the joint probability $p({\\bf X}, {\\bf Z}) = p({\\bf x}_1, \\ldots, {\\bf x}_N, {\\bf z}_1, \\ldots, {\\bf z}_N)$ of a hidden Markov model (HMM) can be compute as\n",
    "\n",
    "$$\n",
    "p({\\bf x}_1, \\ldots, {\\bf x}_N, {\\bf z}_1, \\ldots, {\\bf z}_N) = p({\\bf z}_1) \n",
    "\\left[ \\prod_{n=2}^N p({\\bf z}_n \\mid {\\bf z}_{n-1}) \\right]\n",
    "\\prod_{n=1}^N p({\\bf x}_n \\mid {\\bf z}_n)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing without log-transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `joint_prob` given a model (e.g. in the representation above) and sequence of observables, ${\\bf X}$, and a sequence of hidden states, ${\\bf Z}$, computes the joint probability cf. the above formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "nbpresent": {
     "id": "3769ed05-effc-42d7-ae3b-655ea2082dc3"
    }
   },
   "outputs": [],
   "source": [
    "def joint_prob(model, x, z):\n",
    "    \"\"\"\n",
    "    Expects that x and y are indices in a list\n",
    "    \"\"\"\n",
    "    p_1 = model.init_probs[z[0]]\n",
    "    trans = 1\n",
    "    emission = 1\n",
    "    for i in range(1, len(z)):\n",
    "        trans = trans * model.trans_probs[z[i-1]][z[i]]\n",
    "    for d in range(len(x)):\n",
    "        emission = emission * model.emission_probs[z[d]][x[d]]\n",
    "\n",
    "    return p_1*trans*emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.9 , 0.1 , 0.  , 0.  , 0.  ],\n",
       "       [1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.05, 0.9 , 0.05, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ],\n",
       "       [0.  , 0.  , 0.  , 0.1 , 0.9 , 0.  , 0.  ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm_7_state.trans_probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "bf292319-b283-4947-ba29-8db3efea4541"
    }
   },
   "source": [
    "Now compute the joint probability of the ${\\bf X}$ (`x_short`) and ${\\bf Z}$ (`z_short`) below using the 7-state (`hmm_7_state`) model introduced above. (*Remember to translate them first using the appropriate functions introduces above!*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "nbpresent": {
     "id": "51699e6f-c98d-4552-8d21-5e37153bab84"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9114255184318878e-31"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_short = 'GTTTCCCAGTGTATATCGAGGGATACTACGTGCATAGTAACATCGGCCAA'\n",
    "z_short = '33333333333321021021021021021021021021021021021021'\n",
    "\n",
    "x_indices = translate_observations_to_indices(x_short)\n",
    "z_indices = translate_path_to_indices(z_short)\n",
    "\n",
    "joint_prob(hmm_7_state, x_indices, z_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "62cc8755-8cf4-4b52-9e47-73395c18e741"
    }
   },
   "source": [
    "## Implementing with log-transformation (i.e. in \"log-space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "65813762-66da-4d50-8290-197eb23d5c90"
    }
   },
   "source": [
    "Now implement the joint probability function using log-transformation as explained in the lecture. We've given you a log-function that handles $\\log(0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "nbpresent": {
     "id": "c4e0a0e0-1fec-462d-89ad-3797ba7d24f3"
    }
   },
   "outputs": [],
   "source": [
    "def log(x):\n",
    "    if x == 0:\n",
    "        return float('-inf')\n",
    "    return math.log(x)\n",
    "\n",
    "def joint_prob_log(model, x, z): # 30 in basic lec (den første)\n",
    "    p_1 = log(model.init_probs[z[0]])\n",
    "    trans = 0\n",
    "    emission = 0\n",
    "    for i in range(1, len(z)):\n",
    "        trans += log(model.trans_probs[z[i-1]][z[i]])\n",
    "    for d in range(len(x)):\n",
    "        emission += log(model.emission_probs[z[d]][x[d]])\n",
    "        \n",
    "    return p_1+trans+emission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d47a1d9b-de76-41cc-88de-694965dd1ce7"
    }
   },
   "source": [
    "Confirm that the log-transformed function is correct by comparing the output of `joint_prob_log` to the output of `joint_prob`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9114255184319232e-31"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(joint_prob_log(hmm_7_state, x_indices, z_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "06151990-65f6-401e-aa31-7adc39603c4b"
    }
   },
   "source": [
    "## Comparison of Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "10ae159d-95f8-47aa-a42b-6bda8e25a0a2"
    }
   },
   "source": [
    "Now that you have two ways to compute the joint probability given a model, a sequence of observations, and a sequence of hidden states, try to make an experiment to figure out how long a sequence can be before it becomes essential to use the log-transformed version. For this experiment we'll use two longer sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_long = 'TGAGTATCACTTAGGTCTATGTCTAGTCGTCTTTCGTAATGTTTGGTCTTGTCACCAGTTATCCTATGGCGCTCCGAGTCTGGTTCTCGAAATAAGCATCCCCGCCCAAGTCATGCACCCGTTTGTGTTCTTCGCCGACTTGAGCGACTTAATGAGGATGCCACTCGTCACCATCTTGAACATGCCACCAACGAGGTTGCCGCCGTCCATTATAACTACAACCTAGACAATTTTCGCTTTAGGTCCATTCACTAGGCCGAAATCCGCTGGAGTAAGCACAAAGCTCGTATAGGCAAAACCGACTCCATGAGTCTGCCTCCCGACCATTCCCATCAAAATACGCTATCAATACTAAAAAAATGACGGTTCAGCCTCACCCGGATGCTCGAGACAGCACACGGACATGATAGCGAACGTGACCAGTGTAGTGGCCCAGGGGAACCGCCGCGCCATTTTGTTCATGGCCCCGCTGCCGAATATTTCGATCCCAGCTAGAGTAATGACCTGTAGCTTAAACCCACTTTTGGCCCAAACTAGAGCAACAATCGGAATGGCTGAAGTGAATGCCGGCATGCCCTCAGCTCTAAGCGCCTCGATCGCAGTAATGACCGTCTTAACATTAGCTCTCAACGCTATGCAGTGGCTTTGGTGTCGCTTACTACCAGTTCCGAACGTCTCGGGGGTCTTGATGCAGCGCACCACGATGCCAAGCCACGCTGAATCGGGCAGCCAGCAGGATCGTTACAGTCGAGCCCACGGCAATGCGAGCCGTCACGTTGCCGAATATGCACTGCGGGACTACGGACGCAGGGCCGCCAACCATCTGGTTGACGATAGCCAAACACGGTCCAGAGGTGCCCCATCTCGGTTATTTGGATCGTAATTTTTGTGAAGAACACTGCAAACGCAAGTGGCTTTCCAGACTTTACGACTATGTGCCATCATTTAAGGCTACGACCCGGCTTTTAAGACCCCCACCACTAAATAGAGGTACATCTGA'\n",
    "z_long = '3333321021021021021021021021021021021021021021021021021021021021021021033333333334564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564563210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210321021021021021021021021033334564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564563333333456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456332102102102102102102102102102102102102102102102102102102102102102102102102102102102102102102102103210210210210210210210210210210210210210210210210210210210210210'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "85880ba1-bc5b-455a-9335-000678ea9b0e"
    }
   },
   "source": [
    "Now compute the joint probability with `joint_prob` the 7-state (hmm_7_state) model introduced above, and see when it breaks (i.e. when it wrongfully becomes 0). Does this make sense? Here's some code to get you started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "nbpresent": {
     "id": "c558e928-6660-4b69-8db3-3341e8b19dba"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.861952429010216e-65\n",
      "1.6175774997005766e-122\n",
      "3.0675430597843052e-183\n",
      "4.86070414430298e-247\n",
      "5.2587243422067335e-306\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "x_indices = translate_observations_to_indices(x_long)\n",
    "z_indices = translate_path_to_indices(z_long)\n",
    "\n",
    "for i in range(100, len(x_long)+1, 100):\n",
    "    x = x_indices[:i]\n",
    "    z = z_indices[:i]\n",
    "    \n",
    "    #print(len(x_trans))\n",
    "    jp = joint_prob(hmm_7_state, x, z)\n",
    "    if jp == 0:\n",
    "        print(i)\n",
    "    else: \n",
    "        print(jp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below you should state for which $i$ computing the joint probability (for the two models considered) using `joint_prob` wrongfully becomes 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here:**\n",
    "\n",
    "For the 7-state model, `joint_prob` becomes 0 for **i = ?**.\n",
    "\n",
    "Ved 600 elementer bliver den til 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4 - Viterbi Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you will implement and experiment with the Viterbi algorithm. The implementation has been split into three parts:\n",
    "\n",
    "1. Fill out the $\\omega$ table using the recursion presented at the lecture.\n",
    "2. Find the state with the highest probability after observing the entire sequence of observations.\n",
    "3. Backtrack from the state found in the previous step to obtain the optimal path.\n",
    "\n",
    "We'll be working with the 7-state model (`hmm_7_state`) and the helper function for translating between observations, hidden states, and indicies, as introduced above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, you're given the function below that constructs a table of a specific size filled with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_table(m, n):\n",
    "    \"\"\"Make a table with `m` rows and `n` columns filled with zeros.\"\"\"\n",
    "    return np.zeros((m, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll be testing your code with the same two sequences as above, i.e:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_short = 'GTTTCCCAGTGTATATCGAGGGATACTACGTGCATAGTAACATCGGCCAA'\n",
    "z_short = '33333333333321021021021021021021021021021021021021'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_long = 'TGAGTATCACTTAGGTCTATGTCTAGTCGTCTTTCGTAATGTTTGGTCTTGTCACCAGTTATCCTATGGCGCTCCGAGTCTGGTTCTCGAAATAAGCATCCCCGCCCAAGTCATGCACCCGTTTGTGTTCTTCGCCGACTTGAGCGACTTAATGAGGATGCCACTCGTCACCATCTTGAACATGCCACCAACGAGGTTGCCGCCGTCCATTATAACTACAACCTAGACAATTTTCGCTTTAGGTCCATTCACTAGGCCGAAATCCGCTGGAGTAAGCACAAAGCTCGTATAGGCAAAACCGACTCCATGAGTCTGCCTCCCGACCATTCCCATCAAAATACGCTATCAATACTAAAAAAATGACGGTTCAGCCTCACCCGGATGCTCGAGACAGCACACGGACATGATAGCGAACGTGACCAGTGTAGTGGCCCAGGGGAACCGCCGCGCCATTTTGTTCATGGCCCCGCTGCCGAATATTTCGATCCCAGCTAGAGTAATGACCTGTAGCTTAAACCCACTTTTGGCCCAAACTAGAGCAACAATCGGAATGGCTGAAGTGAATGCCGGCATGCCCTCAGCTCTAAGCGCCTCGATCGCAGTAATGACCGTCTTAACATTAGCTCTCAACGCTATGCAGTGGCTTTGGTGTCGCTTACTACCAGTTCCGAACGTCTCGGGGGTCTTGATGCAGCGCACCACGATGCCAAGCCACGCTGAATCGGGCAGCCAGCAGGATCGTTACAGTCGAGCCCACGGCAATGCGAGCCGTCACGTTGCCGAATATGCACTGCGGGACTACGGACGCAGGGCCGCCAACCATCTGGTTGACGATAGCCAAACACGGTCCAGAGGTGCCCCATCTCGGTTATTTGGATCGTAATTTTTGTGAAGAACACTGCAAACGCAAGTGGCTTTCCAGACTTTACGACTATGTGCCATCATTTAAGGCTACGACCCGGCTTTTAAGACCCCCACCACTAAATAGAGGTACATCTGA'\n",
    "z_long = '3333321021021021021021021021021021021021021021021021021021021021021021033333333334564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564563210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210321021021021021021021021033334564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564563333333456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456332102102102102102102102102102102102102102102102102102102102102102102102102102102102102102102102103210210210210210210210210210210210210210210210210210210210210210'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to translate these sequences to indices before using them with your algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing without log-transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will implement the algorithm without log-transformation. This will cause issues with numerical stability (like above when computing the joint probability), so we will use the log-transformation trick to fix this in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation of the $\\omega$ table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_w(model, x):\n",
    "    \"\"\"\n",
    "    Assumes x to be a list of integers\n",
    "    Implemented as in ml-3 slides slide 4\n",
    "    \"\"\"\n",
    "    K = len(model.init_probs) # number of hidden states\n",
    "    N = len(x) # number observations\n",
    "    \n",
    "    w = make_table(K, N)\n",
    "    pi = model.init_probs\n",
    "    A = model.trans_probs\n",
    "    Ø = model.emission_probs\n",
    "\n",
    "    for i in range(K):\n",
    "        w[i,0] = pi[i] * Ø[i, x[0]]\n",
    "    # Inductive case: fill out w[i][j] for i = 0..k, j = 0..n-1\n",
    "    # ...\n",
    "    for j in range(1, N):\n",
    "        for i in range(K):\n",
    "            maximizing = np.max(np.multiply(w[:,j-1], A[:,i]))\n",
    "            w[i,j] = Ø[i, x[j]] * maximizing\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_indices_s = translate_observations_to_indices(x_short)\n",
    "z_indices_s = translate_path_to_indices(z_short)\n",
    "\n",
    "x_indices_l = translate_observations_to_indices(x_long)\n",
    "z_indices_l = translate_path_to_indices(z_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the joint probability of an optimal path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, write a function that given the $\\omega$-table, returns the probability of an optimal path through the HMM. As explained in the lecture, this corresponds to finding the highest probability in the last column of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_path_prob(w):\n",
    "    return np.max(w[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test your implementation in the box below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9114255184318858e-31"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = compute_w(hmm_7_state, x_indices_s)\n",
    "opt_path_prob(w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same for `x_long`. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here ...\n",
    "w = compute_w(hmm_7_state, x_indices_l)\n",
    "opt_path_prob(w) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining an optimal path through backtracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement backtracking to find a most probable path of hidden states given the $\\omega$-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML lec 3 slide 11 ?? \n",
    "def backtrack(model, x, w):\n",
    "    N = len(x)\n",
    "    Ø = model.emission_probs\n",
    "    A = model.trans_probs\n",
    "    \n",
    "    out = np.zeros((1,N))\n",
    "    out[-1] = np.argmax(w[-1], axis = 0)\n",
    "    \n",
    "    \n",
    "    backward_w = w[::-1]\n",
    "    \n",
    "    for c in range(1, N-1)[::-1]:\n",
    "        out[0][c] = np.argmax(Ø[int(out[0][c+1]), x[c+1]] * w[:, c] * A[:, int(out[0][c+1])], axis = 0) \n",
    "    \n",
    "    return out[::-1]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3., 3., 3., 3., 3., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1.,\n",
       "        0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0.,\n",
       "        2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2.,\n",
       "        1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1.,\n",
       "        0., 2., 1., 0., 2., 1., 0., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "        3., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6.,\n",
       "        4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4.,\n",
       "        5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5.,\n",
       "        6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6.,\n",
       "        4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4.,\n",
       "        5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5.,\n",
       "        6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6.,\n",
       "        4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4.,\n",
       "        5., 6., 3., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2.,\n",
       "        1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1.,\n",
       "        0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0.,\n",
       "        2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2.,\n",
       "        1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1.,\n",
       "        0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0.,\n",
       "        2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2.,\n",
       "        1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1.,\n",
       "        0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0.,\n",
       "        2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2.,\n",
       "        1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1.,\n",
       "        0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0.,\n",
       "        2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2.,\n",
       "        1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1.,\n",
       "        0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0.,\n",
       "        2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2.,\n",
       "        1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1.,\n",
       "        0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0.,\n",
       "        2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2.,\n",
       "        1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 3.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = compute_w(hmm_7_state, x_indices_l)\n",
    "backtrack(hmm_7_state, x_indices_l, w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same for `x_long`. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3., 3., 3., 3., 3., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1.,\n",
       "        0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0.,\n",
       "        2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2.,\n",
       "        1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1.,\n",
       "        0., 2., 1., 0., 2., 1., 0., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "        3., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6.,\n",
       "        4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4.,\n",
       "        5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5.,\n",
       "        6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6.,\n",
       "        4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4.,\n",
       "        5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5.,\n",
       "        6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6.,\n",
       "        4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4.,\n",
       "        5., 6., 3., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2.,\n",
       "        1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1.,\n",
       "        0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0.,\n",
       "        2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2.,\n",
       "        1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1.,\n",
       "        0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0.,\n",
       "        2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2.,\n",
       "        1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1.,\n",
       "        0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0.,\n",
       "        2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2.,\n",
       "        1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1.,\n",
       "        0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0.,\n",
       "        2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2.,\n",
       "        1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1.,\n",
       "        0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0.,\n",
       "        2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2.,\n",
       "        1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1.,\n",
       "        0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0.,\n",
       "        2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2.,\n",
       "        1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 3.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here ...\n",
    "w = compute_w(hmm_7_state, x_indices_l)\n",
    "backtrack(hmm_7_state, x_indices_l, w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing with log-transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement the Viterbi algorithm with log-transformation. The steps are the same as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation of the (log-transformed) $\\omega$ table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def log(x):\n",
    "    if x == 0:\n",
    "        return float('-inf')\n",
    "    return math.log(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def log_np(X):\n",
    "    def log(x):\n",
    "        if x == 0:\n",
    "            return float('-inf')\n",
    "        return math.log(x)\n",
    "    return np.array(list(map(log, X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_w_log(model, x):\n",
    "    \"\"\"\n",
    "    Assumes x to be a list of integers\n",
    "    Implemented in logs-space see ml-3 slides slide 6\n",
    "    \"\"\"\n",
    "    K = len(model.init_probs)\n",
    "    N = len(x)\n",
    "    \n",
    "    w = make_table(K, N)\n",
    "    pi = model.init_probs\n",
    "    A = model.trans_probs\n",
    "    Ø = model.emission_probs\n",
    "    \n",
    "    for i in range(K):\n",
    "        w[i,0] = log(pi[i]) + log(Ø[i, x[0]])\n",
    "    # Inductive case: fill out w[i][j] for i = 0..k, j = 0..n-1\n",
    "    # ...\n",
    "    for j in range(1, N):\n",
    "        for i in range(K):\n",
    "            maximizing = np.max(np.add(w[:,j-1], log_np(A[:,i])))\n",
    "            w[i,j] = log(Ø[i, x[j]]) + maximizing\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the (log-transformed) joint probability of an optimal path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_path_prob_log(w):\n",
    "    return np.max(w[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-70.73228857440488"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = compute_w_log(hmm_7_state, x_indices_s)\n",
    "opt_path_prob_log(w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same for `x_long`. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1406.7209253880144"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here ...\n",
    "w = compute_w_log(hmm_7_state, x_indices_l)\n",
    "opt_path_prob_log(w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining an optimal path through backtracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML lec 3 slide 11\n",
    "def backtrack_log(model, x, w):\n",
    "    N = len(x)\n",
    "    Ø = model.emission_probs\n",
    "    A = model.trans_probs\n",
    "    \n",
    "    out = np.zeros((1,N))\n",
    "    out[-1] = np.argmax(w[-1], axis = 0)\n",
    "    \n",
    "    for c in range(1, N-1)[::-1]:\n",
    "        out[0][c] = np.argmax(log(Ø[int(out[0][c+1]), x[c+1]]) + w[:, c] + log_np(A[:, int(out[0][c+1])]), axis = 0) \n",
    "    \n",
    "    return out[::-1]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 2., 1., 0., 2.,\n",
       "        1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1.,\n",
       "        0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0.,\n",
       "        3., 3.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = compute_w_log(hmm_7_state, x_indices_s)\n",
    "backtrack_log(hmm_7_state, x_indices_s, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same for `x_long`. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3., 3., 3., 3., 3., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1.,\n",
       "        0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0.,\n",
       "        2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2.,\n",
       "        1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1.,\n",
       "        0., 2., 1., 0., 2., 1., 0., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "        3., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6.,\n",
       "        4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4.,\n",
       "        5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5.,\n",
       "        6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6.,\n",
       "        4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4.,\n",
       "        5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5.,\n",
       "        6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6.,\n",
       "        4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4.,\n",
       "        5., 6., 3., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2.,\n",
       "        1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1.,\n",
       "        0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0.,\n",
       "        2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2.,\n",
       "        1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1.,\n",
       "        0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0.,\n",
       "        2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2.,\n",
       "        1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1.,\n",
       "        0., 2., 1., 0., 3., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1.,\n",
       "        0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 3., 3., 3.,\n",
       "        3., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6.,\n",
       "        4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4.,\n",
       "        5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5.,\n",
       "        6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6.,\n",
       "        4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4.,\n",
       "        5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5.,\n",
       "        6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6.,\n",
       "        4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4.,\n",
       "        5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5.,\n",
       "        6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6.,\n",
       "        4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4.,\n",
       "        5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5.,\n",
       "        6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6.,\n",
       "        4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4.,\n",
       "        5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5.,\n",
       "        6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6.,\n",
       "        4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4.,\n",
       "        5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5.,\n",
       "        6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6.,\n",
       "        4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4.,\n",
       "        5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5.,\n",
       "        6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 3., 3., 3.,\n",
       "        3., 3., 3., 3., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6.,\n",
       "        4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4.,\n",
       "        5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5.,\n",
       "        6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6.,\n",
       "        4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4.,\n",
       "        5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5.,\n",
       "        6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6., 4., 5., 6.,\n",
       "        4., 5., 6., 4., 5., 6., 3., 3., 2., 1., 0., 2., 1., 0., 2., 1.,\n",
       "        0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0.,\n",
       "        2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2.,\n",
       "        1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1.,\n",
       "        0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0.,\n",
       "        2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2.,\n",
       "        1., 0., 2., 1., 0., 2., 1., 0., 3., 2., 1., 0., 2., 1., 0., 2.,\n",
       "        1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1.,\n",
       "        0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0.,\n",
       "        2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2., 1., 0., 2.,\n",
       "        1., 0., 2., 1., 0., 3., 3., 3.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here ...\n",
    "w = compute_w_log(hmm_7_state, x_indices_l)\n",
    "backtrack_log(hmm_7_state, x_indices_l, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about how to verify that your implementations of Viterbi (i.e. `compute_w`, `opt_path_prob`, `backtrack`, and there log-transformed variants `compute_w_log`, `opt_path_prob_log`, `backtrack_log`) are correct.\n",
    "\n",
    "One thing that should hold is that the probability of a most likely path as computed by `opt_path_prob` (or `opt_path_prob_log`) for a given sequence of observables (e.g. `x_short` or `x_long`) should be equal to the joint probability of a corersponding most probable path as found by `backtrack` (or `backtrack_log`) and the given sequence of observables. Why?\n",
    "\n",
    "Make an experiment that validates that this is the case for your implementations of Viterbi and `x_short` and `x_long`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_adj(A): #Input transition matrix and get in adj list \n",
    "    K = len(A)\n",
    "    Adj = []\n",
    "    \n",
    "    for i in range(K):\n",
    "        Adj.append([])\n",
    "        for j in range(K):\n",
    "            if log(A[j, i]) != float('-inf'):\n",
    "                Adj[i].append((j, A[j, i]))\n",
    "    \n",
    "    return Adj\n",
    "\n",
    "def compute_w_log_quick(model, x):\n",
    "    \"\"\"\n",
    "    Assumes x to be a list of integers\n",
    "    Implemented in logs-space see ml-3 slides slide 6\n",
    "    Quicker as we do not make as many table look-ups \n",
    "    \"\"\"\n",
    "    K = len(model.init_probs)\n",
    "    N = len(x)\n",
    "    \n",
    "    w = make_table(K, N)\n",
    "    pi = model.init_probs\n",
    "    A = model.trans_probs\n",
    "    Ø = model.emission_probs\n",
    "    \n",
    "    Adj = trans_adj(A)\n",
    "    \n",
    "    for i in range(K):\n",
    "        w[i,0] = log(pi[i]) + log(Ø[i, x[0]])\n",
    "    # Inductive case: fill out w[i][j] for i = 0..k, j = 0..n-1\n",
    "    # ...\n",
    "    \n",
    "    for j in range(1, N):\n",
    "        for i in range(K):\n",
    "            if log(Ø[i, x[j]]) != float('-inf'):\n",
    "                noninf = list(zip(*Adj[i]))\n",
    "                \n",
    "                maximizing = np.max(np.add(w[noninf[0],j-1], log_np(noninf[1])))\n",
    "                #print(maximizing, (i, j))\n",
    "                w[i,j] = log(Ø[i, x[j]]) + maximizing\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-log/short:  1.9114255184318858e-31 1.9114255184318878e-31\n",
      "log/short:  -70.73228857440488 -70.73228857440488 -70.73228857440485\n",
      "non-log/long:  0.0 0.0\n",
      "log/long:  -1406.7209253880144 -1406.7209253880144 -1406.7209253880158\n"
     ]
    }
   ],
   "source": [
    "# Check that opt_path_prob is equal to joint_prob(hmm_7_state, x_short, z_viterbi)\n",
    "# Your code here ...\n",
    "w = compute_w(hmm_7_state, x_indices_s)\n",
    "o = opt_path_prob(w)\n",
    "\n",
    "jp = joint_prob(hmm_7_state, x_indices_s, z_indices_s)\n",
    "print(\"non-log/short: \", o, jp)\n",
    "\n",
    "# Check that opt_path_prob_log is equal to joint_prob_log(hmm_7_state, x_short, z_viterbi_log)\n",
    "\n",
    "# Your code here ...\n",
    "w = compute_w_log(hmm_7_state, x_indices_s)\n",
    "o_log = opt_path_prob_log(w)\n",
    "\n",
    "w_quick = compute_w_log_quick(hmm_7_state, x_indices_s)\n",
    "o_log_quick = opt_path_prob_log(w_quick)\n",
    "\n",
    "jp_log = joint_prob_log(hmm_7_state, x_indices_s, z_indices_s)\n",
    "print(\"log/short: \", o_log, o_log_quick, jp_log)\n",
    "\n",
    "# Do the above checks for x_long ...\n",
    "\n",
    "# Your code here ...\n",
    "w = compute_w(hmm_7_state, x_indices_l)\n",
    "o_l = opt_path_prob(w)\n",
    "\n",
    "jp_l = joint_prob(hmm_7_state, x_indices_l, z_indices_l)\n",
    "print(\"non-log/long: \", o_l, jp_l)\n",
    "\n",
    "#log/long\n",
    "w = compute_w_log(hmm_7_state, x_indices_l)\n",
    "o_log_l = opt_path_prob_log(w)\n",
    "\n",
    "w_quick_l = compute_w_log_quick(hmm_7_state, x_indices_l)\n",
    "o_log_quick_l = opt_path_prob_log(w_quick_l)\n",
    "\n",
    "jp_log_l = joint_prob_log(hmm_7_state, x_indices_l, z_indices_l)\n",
    "print(\"log/long: \", o_log_l, o_log_quick_l,  jp_log_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do your implementations pass the above checks?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
